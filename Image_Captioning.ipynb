{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Automatic Image Captioning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b1f3656156341589b088af629bd1b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7ecd75e1289b4b4d97abea115a2e3cf6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a1093fadcb3c4e619fa85bc3b211b6ec",
              "IPY_MODEL_365ce4077be1418d83e2dcc5d60d80c2"
            ]
          }
        },
        "7ecd75e1289b4b4d97abea115a2e3cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1093fadcb3c4e619fa85bc3b211b6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3270bf263c4e46f1a5117b04753d909c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 20000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3efb2b2484c24a1db57524b4642332eb"
          }
        },
        "365ce4077be1418d83e2dcc5d60d80c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_465a30fc846440e7847a25751d1118eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 20000/20000 [2:29:42&lt;00:00,  2.04it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_701d1a4e99b845c5b218c35ad4e8fb3e"
          }
        },
        "3270bf263c4e46f1a5117b04753d909c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3efb2b2484c24a1db57524b4642332eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "465a30fc846440e7847a25751d1118eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "701d1a4e99b845c5b218c35ad4e8fb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4d7815a5b3c44b29de6d5508474c4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_17a0a7d47d304f438760d9e7d392c8fe",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_899d4f1e82564de88b6542e283ed9ef2",
              "IPY_MODEL_4286b5ace0494544b9a332f1e74f842c"
            ]
          }
        },
        "17a0a7d47d304f438760d9e7d392c8fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "899d4f1e82564de88b6542e283ed9ef2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a775514c7459420393bc38b3888e9f89",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 5000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 5000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b54ced679fb7459296e7fe49ee349110"
          }
        },
        "4286b5ace0494544b9a332f1e74f842c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e248b0221d54716b75569e03d0842ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 5000/5000 [02:30&lt;00:00, 33.23it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_196a1fe9aafc4f29911564e0f95d7eb6"
          }
        },
        "a775514c7459420393bc38b3888e9f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b54ced679fb7459296e7fe49ee349110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e248b0221d54716b75569e03d0842ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "196a1fe9aafc4f29911564e0f95d7eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f43be8eb51244f2a85d957369883083f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b62ecea314e4419680e63cf8660bf2ff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2dbe40ca2caf430b8c7ab45f6fa0e6dc",
              "IPY_MODEL_687c48ceedd7483f8b3852ca934aba0e"
            ]
          }
        },
        "b62ecea314e4419680e63cf8660bf2ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2dbe40ca2caf430b8c7ab45f6fa0e6dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6f7a4041f6164026aa53b51fafe44159",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 20000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dba26a01572e4972aaca9907c88f1d3a"
          }
        },
        "687c48ceedd7483f8b3852ca934aba0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6ec40056916847b1ae3b884e2a4848e6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20000/20000 [00:07&lt;00:00, 2521.37it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0149334e86464d45a26b84c1b43b8975"
          }
        },
        "6f7a4041f6164026aa53b51fafe44159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dba26a01572e4972aaca9907c88f1d3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ec40056916847b1ae3b884e2a4848e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0149334e86464d45a26b84c1b43b8975": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/princetyagitech/Computer_Vision/blob/master/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usNsjj-YiAQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm_notebook\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Bidirectional,LSTM, Embedding, TimeDistributed, Dense, RepeatVector,Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.layers import add\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx72oC-wizLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td5jL-9tiAQq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "c887a7bc-5cbf-4c53-aa8e-809dcc46e883"
      },
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "filename = '/content/drive/My Drive/Image_captioning/Data/flickr30k_images/results.csv'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "print(doc[:300])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "image_name| comment_number| comment\n",
            "1000092795.jpg| 0| Two young guys with shaggy hair look at their hands while hanging out in the yard .\n",
            "1000092795.jpg| 1| Two young , White males are outside near many bushes .\n",
            "1000092795.jpg| 2| Two men in green shirts are standing in a yard .\n",
            "1000092795.jpg| 3| \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE7R0czhiWWD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "f03ef1ea-9827-4552-cac6-f602276217f5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uMHS76HiAQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c408bcb-1817-4720-cc16-5b1d2a31be6a"
      },
      "source": [
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# extract filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# create the list if needed\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\t# store description\n",
        "\t\tmapping[image_id].append(image_desc[3:])\n",
        "\treturn mapping\n",
        "\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 31784 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1wG-x6yVpBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.image as mpimg\n",
        "img=mpimg.imread(images+'1000268201'+'.jpg')\n",
        "plt.imshow(img)\n",
        "descriptions['1000268201']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAjuOdfNiARj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#del descriptions['image_name|']\n",
        "list(descriptions.keys())[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF7eJGvoiARz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "descriptions['1000092795']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qM_2nW5iASC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "e8a6795d-b015-4788-cbae-6116f2b7209d"
      },
      "source": [
        "descriptions['10002456']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Several men in hard hats are operating a giant pulley system .',\n",
              " 'Workers look down from up above on a piece of equipment .',\n",
              " 'Two men working on a machine wearing hard hats .',\n",
              " 'Four men on top of a tall structure .',\n",
              " 'Three men on a large rig .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h7aa6BliAST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# tokenize\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# convert to lower case\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# remove hanging 's' and 'a'\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
        "\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaf7RymwiASg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "39403776-8001-40ec-9af6-b782e1f72d27"
      },
      "source": [
        "descriptions['10002456']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['several men in hard hats are operating giant pulley system',\n",
              " 'workers look down from up above on piece of equipment',\n",
              " 'two men working on machine wearing hard hats',\n",
              " 'four men on top of tall structure',\n",
              " 'three men on large rig']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKiiZaqDiASu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "9d04578b-b154-409f-95c2-93f38083be6d"
      },
      "source": [
        "descriptions['1000092795']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['two young guys with shaggy hair look at their hands while hanging out in the yard',\n",
              " 'two young white males are outside near many bushes',\n",
              " 'two men in green shirts are standing in yard',\n",
              " 'man in blue shirt standing in garden',\n",
              " 'two friends enjoy time spent together']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGa38ym0iAS6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "91af6edf-873d-4b0c-a2d6-7901c29f26db"
      },
      "source": [
        "# convert the loaded descriptions into a vocabulary of words\n",
        "def to_vocabulary(descriptions):\n",
        "\t# build a list of all description strings\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# summarize vocabulary\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Original Vocabulary Size: %d' % len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Vocabulary Size: 19737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbdaqmhGiATO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save descriptions to file, one per line\n",
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or-0iO7diATc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a pre-defined list of photo identifiers\n",
        "def load_set(filename):\n",
        "\tdoc = load_doc(filename)\n",
        "\tdataset = list()\n",
        "\t# process line by line\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# skip empty lines\n",
        "\t\tif len(line) < 1:\n",
        "\t\t\tcontinue\n",
        "\t\t# get the image identifier\n",
        "\t\tidentifier = line.split('.')[0]\n",
        "\t\tdataset.append(identifier)\n",
        "\treturn set(dataset)\n",
        "\n",
        "# load training dataset (6K)\n",
        "filename = '../../storage/image_caption/dataset/TextFiles/Flickr_8k.trainImages.txt'\n",
        "train = load_set(filename)\n",
        "print('Dataset: %d' % len(train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgkBVpFjiATq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Below path contains all the images\n",
        "images = '/content/drive/My Drive/Image_captioning/Data/flickr30k_images/flickr30k_images/'\n",
        "# Create a list of all image names in the directory\n",
        "img = glob.glob(images + '*.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UQZ-K3rlwTR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "322fb5d0-acd1-45ab-aa20-b33f83319a23"
      },
      "source": [
        "len(img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf4lsKX_NXw9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ec4aef65-6d6a-4055-e84b-a499156a6bba"
      },
      "source": [
        "\n",
        "img[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Image_captioning/Data/flickr30k_images/flickr30k_images/5696547236.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MycWk_cenDtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=[]\n",
        "test=[]\n",
        "for i in range(20000):\n",
        "  train.append(img[i].split('/')[8])\n",
        "for i in range(20000,25000):\n",
        "  test.append(img[i].split('/')[8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPnv6cyVNafO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fcdd9acb-bbce-4800-e9a9-eac265074693"
      },
      "source": [
        "train[0],test[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('5696547236.jpg', '105493603.jpg')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3eRVgUEiAT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Below file conatains the names of images to be used in train data\n",
        "#train_images_file = '../../storage/image_caption/dataset/TextFiles/Flickr_8k.trainImages.txt'\n",
        "# Read the train image names in a set\n",
        "train_images = img[:20000]\n",
        "\n",
        "# Create a list of all the training images with their full path names\n",
        "train_img = []\n",
        "\n",
        "for i in img: # img is list of full path names of all images\n",
        "    if i[len(images):] in train_images: # Check if the image belongs to training set\n",
        "        train_img.append(i) # Add it to the list of train images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiyJbtt6QyzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_img=train_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i63XRFqiAUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Below file conatains the names of images to be used in test data\n",
        "#test_images_file = '../../storage/image_caption/dataset/TextFiles/Flickr_8k.testImages.txt'\n",
        "# Read the validation image names in a set# Read the test image names in a set\n",
        "test_images = img[20000:25000]\n",
        "\n",
        "# Create a list of all the test images with their full path names\n",
        "test_img = []\n",
        "\n",
        "for i in img: # img is list of full path names of all images\n",
        "    if i[len(images):] in test_images: # Check if the image belongs to test set\n",
        "        test_img.append(i) # Add it to the list of test images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trC9wrxWRKFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_img=test_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v5rPYniOIv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGhPf5t2iAUu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e31d99a2-419d-4bdb-f36a-7e3efbf04e2d"
      },
      "source": [
        "# load clean descriptions into memory\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# load document\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\t# split id from description\n",
        "\t\t\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t\n",
        "\t\t#print(image_id)\n",
        "\t\t# skip images not in the set\n",
        "\t\tif image_id+'.jpg' in dataset:\n",
        "\t\t\t# create list\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# wrap description in tokens\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# store\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# descriptions\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Descriptions: train=20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kQY8lodiAVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(image_path):\n",
        "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    # Convert PIL image to numpy array of 3-dimensions\n",
        "    x = image.img_to_array(img)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # preprocess the images using preprocess_input() from inception module\n",
        "    x = preprocess_input(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4oiIkQMiAVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the inception v3 model\n",
        "model1 = InceptionV3(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydqv-o9FiAVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
        "model_new = Model(model1.input, model1.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T_azu_ViAWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to encode a given image into a vector of size (2048, )\n",
        "def encode(image):\n",
        "    image = preprocess(image) # preprocess the image\n",
        "    fea_vec = model_new.predict(image) # Get the encoding vector for the image\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return fea_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTCLihIiAWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "7b1f3656156341589b088af629bd1b59",
            "7ecd75e1289b4b4d97abea115a2e3cf6",
            "a1093fadcb3c4e619fa85bc3b211b6ec",
            "365ce4077be1418d83e2dcc5d60d80c2",
            "3270bf263c4e46f1a5117b04753d909c",
            "3efb2b2484c24a1db57524b4642332eb",
            "465a30fc846440e7847a25751d1118eb",
            "701d1a4e99b845c5b218c35ad4e8fb3e"
          ]
        },
        "outputId": "9d1e5314-e698-49d2-f3c5-b748efd3a7c6"
      },
      "source": [
        "# Call the funtion to encode all the train images\n",
        "# This will take a while on CPU - Execute this only once\n",
        "from keras.preprocessing import image\n",
        "start = time()\n",
        "encoding_train = {}\n",
        "for img in tqdm_notebook(train_img):\n",
        "    encoding_train[img[len(images):]] = encode(img)\n",
        "print(\"Time taken in seconds =\", time()-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b1f3656156341589b088af629bd1b59",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Time taken in seconds = 8982.23938536644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FVb0RrriAWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the bottleneck train features to disk\n",
        "import pickle\n",
        "with open(\"/content/drive/My Drive/Image_captioning/encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding_train, encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfUpVYaBiAW_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "b4d7815a5b3c44b29de6d5508474c4e2",
            "17a0a7d47d304f438760d9e7d392c8fe",
            "899d4f1e82564de88b6542e283ed9ef2",
            "4286b5ace0494544b9a332f1e74f842c",
            "a775514c7459420393bc38b3888e9f89",
            "b54ced679fb7459296e7fe49ee349110",
            "4e248b0221d54716b75569e03d0842ef",
            "196a1fe9aafc4f29911564e0f95d7eb6"
          ]
        },
        "outputId": "3ccea0fe-bef8-421c-cf6e-47a791f1f1fb"
      },
      "source": [
        "# Call the funtion to encode all the test images - Execute this only once\n",
        "from keras.preprocessing import image\n",
        "start = time()\n",
        "encoding_test = {}\n",
        "for img in tqdm_notebook(test_img):\n",
        "    encoding_test[img[len(images):]] = encode(img)\n",
        "print(\"Time taken in seconds =\", time()-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4d7815a5b3c44b29de6d5508474c4e2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=5000), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Time taken in seconds = 150.51152992248535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AczYtAvPiAXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the bottleneck test features to disk\n",
        "with open('/content/drive/My Drive/Image_captioning/encoded_test_images.pkl', \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding_test, encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTyjEpFdzI2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "446600f7-bce5-436e-cab1-51442876e277"
      },
      "source": [
        "train_features = load(open('/content/drive/My Drive/Image_captioning/encoded_train_images.pkl', \"rb\"))\n",
        "print('Photos: train=%d' % len(train_features))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Photos: train=20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI_-d6N4iAYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "f43be8eb51244f2a85d957369883083f",
            "b62ecea314e4419680e63cf8660bf2ff",
            "2dbe40ca2caf430b8c7ab45f6fa0e6dc",
            "687c48ceedd7483f8b3852ca934aba0e",
            "6f7a4041f6164026aa53b51fafe44159",
            "dba26a01572e4972aaca9907c88f1d3a",
            "6ec40056916847b1ae3b884e2a4848e6",
            "0149334e86464d45a26b84c1b43b8975"
          ]
        },
        "outputId": "8e0b15ac-b518-4d97-807f-5171f1a358bb"
      },
      "source": [
        "# Create a list of all the training captions\n",
        "all_train_captions = []\n",
        "for key, val in tqdm_notebook(train_descriptions.items()):\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)\n",
        "len(all_train_captions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f43be8eb51244f2a85d957369883083f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=20000), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKUpkKEbiAYh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4f12b35-7b02-41f8-fb8c-4c30fd1b5913"
      },
      "source": [
        "# Consider only words which occur at least 10 times in the corpus\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preprocessed words 16433 -> 4323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiUmJ6YBiAYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1ClvsUZiAZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d1ccc67-b079-4041-af72-3609af6335bc"
      },
      "source": [
        "vocab_size = len(ixtoword) + 1 # one for appended 0's\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4324"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVHP1HFJzNL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e5b4511-90e8-49c9-fd6b-36e645ebff5a"
      },
      "source": [
        "# convert a dictionary of clean descriptions to a list of descriptions\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# calculate the length of the description with the most words\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# determine the maximum sequence length\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Description Length: 74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VEwL2tIiAaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data generator, intended to be used in a call to model.fit_generator()\n",
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # loop for ever over images\n",
        "    while 1:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # retrieve the photo feature\n",
        "            photo = photos[key+'.jpg']\n",
        "            for desc in desc_list:\n",
        "                # encode the sequence\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "                # split one sequence into multiple X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pair\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # store\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            # yield the batch data\n",
        "            if n==num_photos_per_batch:\n",
        "                yield [[array(X1), array(X2)], array(y)]\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so0CWU3fge6g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef248cb-f703-4b7e-b77f-f806585a4c5a"
      },
      "source": [
        "!unzip '/content/drive/My Drive/Image_captioning/glove/glove.6B.200d.txt.zip'  -d '/content/drive/My Drive/Image_captioning/glove'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Image_captioning/glove/glove.6B.200d.txt.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9u8CPPvviAbT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "32f81f21-d68e-4807-c746-cda29790c1df"
      },
      "source": [
        "# Load Glove vectors\n",
        "glove_dir = '/content/drive/My Drive/Image_captioning/glove'\n",
        "embeddings_index = {} # empty dictionary\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyBgtUciiAb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 200\n",
        "\n",
        "# Get 200-dim dense vector for each of the 10000 words in out vocabulary\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in wordtoix.items():\n",
        "    #if i < max_words:\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in the embedding index will be all zeros\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFDCeVyFiAcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2593053a-1550-481a-c699-bf6af033a6b8"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4324, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40N4k0fnc9aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U keras==1.0.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0biQqeMiAdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXmpBDh6iAdW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "1215ca0d-6bd5-4782-913f-b840fb6f8046"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, 74)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 2048)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 74, 200)      864800      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 2048)         0           input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 74, 200)      0           embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          524544      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 256)          467968      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 4324)         1111268     dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,034,372\n",
            "Trainable params: 3,034,372\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LipOWQlPiAdy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7573ef64-f01c-4e65-c1a6-06c55de76b46"
      },
      "source": [
        "model.layers[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.layers.embeddings.Embedding at 0x7f1c7d7c2d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORuKNXNSiAd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoI_ulypiAeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqDUoMOLiAeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "number_pics_per_bath = 125\n",
        "steps = len(train_descriptions)//number_pics_per_bath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMwxPLmkiAfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "    model.save('/content/drive/My Drive/Image_captioning/model_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3ifHu52SpLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.optimizer.lr = 0.0001\n",
        "epochs = 20\n",
        "number_pics_per_bath = 10\n",
        "steps = len(train_descriptions)//number_pics_per_bath"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE28X4ZBiAf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "    model.save('/content/drive/My Drive/Image_captioning/model_' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrXACf3yiAgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('/content/drive/My Drive/Image_captioning/model_30.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIOKHaF_vxYA",
        "colab_type": "text"
      },
      "source": [
        "# Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_9jcTJiAhI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6055f93b-cd85-4bdb-a0f4-c97ecf24d5d1"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model=load_model('/content/drive/My Drive/Image_captioning/model_30.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmKJYxV5t4k7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images = '/content/drive/My Drive/Image_captioning/Data/flickr30k_images/flickr30k_images/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8FywTW7iAiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Image_captioning/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
        "    encoding_test = load(encoded_pickle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew7ni_UbiAiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedySearch(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Z1oF_cIt9z4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(image_path):\n",
        "    # Convert all the images to size 299x299 as expected by the inception v3 model\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    # Convert PIL image to numpy array of 3-dimensions\n",
        "    x = image.img_to_array(img)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # preprocess the images using preprocess_input() from inception module\n",
        "    x = preprocess_input(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tbOFcYMwVT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the inception v3 model\n",
        "model1 = InceptionV3(weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPOjpOEzwbXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
        "model_new = Model(model1.input, model1.layers[-2].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQsl0fIawgJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to encode a given image into a vector of size (2048, )\n",
        "def encode(image):\n",
        "    image = preprocess(image) # preprocess the image\n",
        "    fea_vec = model.predict(image) # Get the encoding vector for the image\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return fea_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPXgGkwzuBLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "test_image_load=encode('/content/drive/My Drive/WhatsApp Image 2020-02-18 at 3.32.44 PM.jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9IxxCWYuDWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length=74\n",
        "pic = list(encoding_test.keys())[1609]\n",
        "image = test_image_load.reshape((1,2048))\n",
        "x=plt.imread('/content/drive/My Drive/WhatsApp Image 2020-02-18 at 3.32.44 PM.jpeg')\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "print(\"Greedy:\",greedySearch(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UldTAKPvuOpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}